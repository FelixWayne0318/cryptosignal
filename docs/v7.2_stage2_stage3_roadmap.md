# v7.2 阶段2/3 详细实施方案

## 目录
1. [阶段2：统计优化（1-2个月）](#阶段2统计优化1-2个月)
2. [阶段3：ML增强（3-6个月）](#阶段3ml增强3-6个月)
3. [数据架构](#数据架构)
4. [实施时间线](#实施时间线)
5. [成功指标](#成功指标)
6. [风险评估](#风险评估)

---

## 阶段2：统计优化（1-2个月）

### 目标
将v7.2阶段1的规则系统，通过真实交易数据反馈，优化为数据驱动的统计系统。

### 核心任务

#### 1. 真实交易数据积累

**1.1 数据采集系统**

```python
# ats_core/data/trade_recorder.py

class TradeRecorder:
    """交易记录器 - 采集真实执行数据"""

    def record_signal_publish(self, signal_data):
        """记录信号发布时刻的快照"""
        record = {
            "signal_id": generate_id(),
            "timestamp": now(),
            "symbol": signal_data['symbol'],
            "side": signal_data['side'],
            "confidence": signal_data['confidence'],

            # 因子快照
            "factors": {
                "T": signal_data['T'],
                "M": signal_data['M'],
                "C": signal_data['C'],
                "V": signal_data['V'],
                "O": signal_data['O'],
                "B": signal_data['B'],
                "F_v2": signal_data['F_v2']
            },

            # 市场状态
            "market_state": {
                "price": signal_data['price'],
                "atr": signal_data['atr'],
                "independence": signal_data['independence'],
                "market_regime": signal_data['market_regime']
            },

            # 预测值
            "predicted": {
                "P_calibrated": signal_data['P_calibrated'],
                "EV_net": signal_data['EV_net'],
                "TP_target": signal_data['TP'],
                "SL_target": signal_data['SL']
            },

            # 执行意图
            "execution_intent": {
                "entry_price_limit": signal_data['entry_price'],
                "expected_cost_bps": signal_data['estimated_cost']
            }
        }

        self.db.insert("signal_snapshots", record)
        return record['signal_id']

    def record_execution(self, signal_id, execution_data):
        """记录实际执行结果"""
        record = {
            "signal_id": signal_id,
            "timestamp": now(),

            # 实际执行
            "actual_entry_price": execution_data['fill_price'],
            "actual_fill_time": execution_data['fill_time'],
            "actual_cost_bps": self._calculate_cost(execution_data),

            # 成本分解
            "cost_breakdown": {
                "spread_bps": execution_data['spread_bps'],
                "slippage_bps": execution_data['slippage_bps'],
                "fee_bps": execution_data['fee_bps'],
                "funding_bps": 0  # 开仓时funding=0
            }
        }

        self.db.insert("execution_records", record)

    def record_trade_result(self, signal_id, result_data):
        """记录交易最终结果"""
        record = {
            "signal_id": signal_id,
            "timestamp": now(),

            # 结果
            "outcome": result_data['outcome'],  # "win", "loss", "breakeven"
            "exit_price": result_data['exit_price'],
            "exit_reason": result_data['reason'],  # "TP", "SL", "timeout"
            "hold_duration_hours": result_data['duration_hours'],

            # PnL
            "pnl_pct": result_data['pnl_pct'],
            "pnl_usdt": result_data['pnl_usdt'],

            # 累计成本
            "total_cost_bps": result_data['total_cost'],  # entry + exit + funding
            "funding_cost_bps": result_data['funding_cost']
        }

        self.db.insert("trade_results", record)
```

**1.2 数据存储设计**

```sql
-- signal_snapshots: 信号快照表
CREATE TABLE signal_snapshots (
    signal_id TEXT PRIMARY KEY,
    timestamp INTEGER,
    symbol TEXT,
    side TEXT,  -- 'long' or 'short'
    confidence REAL,

    -- 因子（JSON）
    factors TEXT,

    -- 市场状态（JSON）
    market_state TEXT,

    -- 预测值（JSON）
    predicted TEXT,

    -- 执行意图（JSON）
    execution_intent TEXT,

    INDEX idx_timestamp (timestamp),
    INDEX idx_symbol (symbol),
    INDEX idx_confidence (confidence)
);

-- execution_records: 执行记录表
CREATE TABLE execution_records (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    signal_id TEXT,
    timestamp INTEGER,
    actual_entry_price REAL,
    actual_fill_time INTEGER,
    actual_cost_bps REAL,
    cost_breakdown TEXT,  -- JSON

    FOREIGN KEY (signal_id) REFERENCES signal_snapshots(signal_id),
    INDEX idx_signal_id (signal_id)
);

-- trade_results: 交易结果表
CREATE TABLE trade_results (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    signal_id TEXT,
    timestamp INTEGER,
    outcome TEXT,  -- 'win', 'loss', 'breakeven'
    exit_price REAL,
    exit_reason TEXT,
    hold_duration_hours REAL,
    pnl_pct REAL,
    pnl_usdt REAL,
    total_cost_bps REAL,
    funding_cost_bps REAL,

    FOREIGN KEY (signal_id) REFERENCES signal_snapshots(signal_id),
    INDEX idx_signal_id (signal_id),
    INDEX idx_outcome (outcome),
    INDEX idx_timestamp (timestamp)
);
```

**存储估算**：
- 每个信号快照：~2KB（JSON压缩）
- 每天信号数：50个（保守估计）
- 60天数据：50 × 60 × 2KB ≈ 6MB
- 加上K线、OI等历史数据：~5GB
- **总计：< 10GB（50GB VPS充足）**

---

#### 2. 校准表优化

**2.1 自适应校准系统**

```python
# ats_core/calibration/adaptive_calibrator.py

class AdaptiveCalibrator:
    """自适应校准器 - 基于真实数据持续优化"""

    def __init__(self, db_path, min_samples_per_bucket=30):
        self.db = Database(db_path)
        self.min_samples = min_samples_per_bucket
        self.calibration_table = {}
        self.metadata = {}

    def update_from_database(self):
        """从数据库重新计算校准表"""
        # 查询所有已完成的交易
        query = """
        SELECT
            ss.confidence,
            tr.outcome
        FROM signal_snapshots ss
        JOIN trade_results tr ON ss.signal_id = tr.signal_id
        WHERE tr.outcome IN ('win', 'loss')
        """

        results = self.db.query(query)

        # 分桶统计
        buckets = defaultdict(lambda: {"wins": 0, "total": 0})

        for row in results:
            confidence = row['confidence']
            bucket = int(confidence // 10) * 10  # 0, 10, 20, ..., 90

            buckets[bucket]['total'] += 1
            if row['outcome'] == 'win':
                buckets[bucket]['wins'] += 1

        # 计算每个桶的真实胜率
        for bucket, stats in buckets.items():
            if stats['total'] >= self.min_samples:
                winrate = stats['wins'] / stats['total']
                self.calibration_table[bucket] = winrate
                self.metadata[bucket] = {
                    "sample_count": stats['total'],
                    "wins": stats['wins'],
                    "confidence_level": "high" if stats['total'] >= 100 else "medium"
                }

        self._save_to_file()
        return self.calibration_table

    def get_calibrated_probability(self, confidence):
        """查表获取校准后的概率"""
        bucket = int(confidence // 10) * 10

        if bucket in self.calibration_table:
            # 使用真实数据
            return self.calibration_table[bucket]
        else:
            # Fallback到bootstrap
            return self._bootstrap_probability(confidence)

    def _bootstrap_probability(self, confidence):
        """冷启动概率（阶段1的线性映射）"""
        P = 0.40 + (confidence / 100.0) * 0.30
        return max(0.35, min(0.75, P))

    def analyze_calibration_quality(self):
        """分析校准质量"""
        analysis = {
            "total_buckets": len(self.calibration_table),
            "total_samples": sum(m['sample_count'] for m in self.metadata.values()),
            "high_confidence_buckets": sum(1 for m in self.metadata.values() if m['confidence_level'] == 'high'),
            "coverage": len(self.calibration_table) / 10.0,  # 最多10个桶
            "buckets": []
        }

        for bucket in sorted(self.calibration_table.keys()):
            analysis['buckets'].append({
                "confidence_range": f"{bucket}-{bucket+10}",
                "true_winrate": self.calibration_table[bucket],
                "sample_count": self.metadata[bucket]['sample_count'],
                "confidence_level": self.metadata[bucket]['confidence_level']
            })

        return analysis
```

**2.2 置信度-胜率映射进化**

| 阶段 | 方法 | 数据需求 | 预期精度 |
|------|------|----------|----------|
| 阶段1 (Day1-7) | Bootstrap线性映射 | 0样本 | ±10% |
| 阶段2 (Week2-4) | 分桶统计（10分位） | 300+样本 | ±5% |
| 阶段2 (Month2) | 分桶统计（5分位） | 1000+样本 | ±3% |
| 阶段3 | Isotonic回归 | 5000+样本 | ±1-2% |

**2.3 校准监控看板**

```python
# ats_core/monitoring/calibration_monitor.py

class CalibrationMonitor:
    """校准质量监控"""

    def generate_report(self):
        """生成校准报告"""
        report = {
            "timestamp": now(),
            "calibration_curve": self._plot_calibration_curve(),
            "brier_score": self._calculate_brier_score(),
            "log_loss": self._calculate_log_loss(),
            "bucket_stats": self._get_bucket_statistics()
        }

        return report

    def _plot_calibration_curve(self):
        """绘制校准曲线（预测概率 vs 真实胜率）"""
        query = """
        SELECT
            ss.confidence,
            CAST(json_extract(ss.predicted, '$.P_calibrated') AS REAL) as predicted_p,
            tr.outcome
        FROM signal_snapshots ss
        JOIN trade_results tr ON ss.signal_id = tr.signal_id
        """

        results = self.db.query(query)

        # 按预测概率分桶
        buckets = defaultdict(lambda: {"predicted": 0, "wins": 0, "total": 0})

        for row in results:
            predicted_p = row['predicted_p']
            bucket = int(predicted_p * 10) / 10.0  # 0.4, 0.5, 0.6, 0.7

            buckets[bucket]['predicted'] = predicted_p
            buckets[bucket]['total'] += 1
            if row['outcome'] == 'win':
                buckets[bucket]['wins'] += 1

        # 计算实际胜率
        curve_data = []
        for bucket in sorted(buckets.keys()):
            stats = buckets[bucket]
            actual_winrate = stats['wins'] / stats['total'] if stats['total'] > 0 else 0
            curve_data.append({
                "predicted": bucket,
                "actual": actual_winrate,
                "sample_count": stats['total']
            })

        return curve_data

    def _calculate_brier_score(self):
        """计算Brier Score（校准质量指标）"""
        query = """
        SELECT
            CAST(json_extract(ss.predicted, '$.P_calibrated') AS REAL) as predicted_p,
            CASE WHEN tr.outcome = 'win' THEN 1 ELSE 0 END as actual
        FROM signal_snapshots ss
        JOIN trade_results tr ON ss.signal_id = tr.signal_id
        """

        results = self.db.query(query)

        brier_sum = 0
        count = 0
        for row in results:
            predicted = row['predicted_p']
            actual = row['actual']
            brier_sum += (predicted - actual) ** 2
            count += 1

        brier_score = brier_sum / count if count > 0 else None

        # Brier Score: 越小越好
        # 0.00 = 完美校准
        # 0.25 = 随机猜测
        return {
            "score": brier_score,
            "interpretation": "excellent" if brier_score < 0.10 else
                            "good" if brier_score < 0.15 else
                            "fair" if brier_score < 0.20 else "poor"
        }
```

---

#### 3. 闸门阈值调整

**3.1 数据驱动的阈值优化**

```python
# ats_core/optimization/gate_optimizer.py

class GateOptimizer:
    """闸门阈值优化器"""

    def optimize_f_gate_threshold(self):
        """优化F闸门阈值"""
        # 当前阈值：F_directional >= -15

        query = """
        SELECT
            CAST(json_extract(ss.factors, '$.F_v2') AS REAL) as F_v2,
            ss.side,
            tr.outcome,
            tr.pnl_pct
        FROM signal_snapshots ss
        JOIN trade_results tr ON ss.signal_id = tr.signal_id
        """

        results = self.db.query(query)

        # 计算F_directional
        data = []
        for row in results:
            F = row['F_v2']
            side_long = (row['side'] == 'long')
            F_dir = F if side_long else -F
            win = (row['outcome'] == 'win')

            data.append({
                "F_dir": F_dir,
                "win": win,
                "pnl": row['pnl_pct']
            })

        # 扫描不同阈值
        thresholds = range(-30, 10, 5)  # -30, -25, -20, -15, -10, -5, 0, 5

        results = []
        for threshold in thresholds:
            # 应用阈值过滤
            filtered = [d for d in data if d['F_dir'] >= threshold]

            if len(filtered) < 50:  # 样本太少，跳过
                continue

            winrate = sum(1 for d in filtered if d['win']) / len(filtered)
            avg_pnl = sum(d['pnl'] for d in filtered) / len(filtered)
            signal_count = len(filtered)

            results.append({
                "threshold": threshold,
                "winrate": winrate,
                "avg_pnl": avg_pnl,
                "signal_count": signal_count,
                "sharpe_proxy": avg_pnl * math.sqrt(signal_count)  # 简化的夏普比率
            })

        # 找到最优阈值（最大化夏普比率）
        best = max(results, key=lambda x: x['sharpe_proxy'])

        return {
            "current_threshold": -15,
            "optimal_threshold": best['threshold'],
            "improvement": {
                "winrate": best['winrate'],
                "avg_pnl": best['avg_pnl'],
                "signal_count": best['signal_count']
            },
            "all_results": results
        }

    def optimize_market_gate(self):
        """优化市场闸门（独立性阈值）"""
        # 当前规则：I < 30 且逆势 → 拒绝

        query = """
        SELECT
            CAST(json_extract(ss.market_state, '$.independence') AS REAL) as independence,
            CAST(json_extract(ss.market_state, '$.market_regime') AS REAL) as market_regime,
            ss.side,
            tr.outcome
        FROM signal_snapshots ss
        JOIN trade_results tr ON ss.signal_id = tr.signal_id
        """

        results = self.db.query(query)

        # 分析：低独立性 + 逆势的表现
        adverse_trades = []
        for row in results:
            I = row['independence']
            market = row['market_regime']
            side_long = (row['side'] == 'long')
            adverse = (market < -30 and side_long) or (market > 30 and not side_long)

            if adverse:
                adverse_trades.append({
                    "I": I,
                    "win": (row['outcome'] == 'win')
                })

        # 按独立性分桶
        I_thresholds = [20, 25, 30, 35, 40]

        analysis = []
        for threshold in I_thresholds:
            filtered = [t for t in adverse_trades if t['I'] < threshold]

            if len(filtered) < 20:
                continue

            winrate = sum(1 for t in filtered if t['win']) / len(filtered)

            analysis.append({
                "I_threshold": threshold,
                "winrate": winrate,
                "sample_count": len(filtered)
            })

        return {
            "current_threshold": 30,
            "recommendation": "保持30" if not analysis else f"调整为{min(analysis, key=lambda x: x['winrate'])['I_threshold']}",
            "analysis": analysis
        }
```

**3.2 闸门效果评估**

| 闸门 | 初始阈值 | 评估周期 | 优化目标 |
|------|----------|----------|----------|
| Gate1 (数据质量) | bars >= 100 | Week 2 | 最小K线数 vs 信号质量 |
| Gate2 (F闸门) | F_dir >= -15 | Week 3-4 | 最大化Sharpe |
| Gate3 (市场闸门) | I < 30且逆势 | Week 4-6 | 逆势交易胜率 |
| Gate4 (成本闸门) | EV > 0 | Week 6-8 | 真实EV vs 预测EV |

---

#### 4. 成本模型精细化

**4.1 真实成本分析**

```python
# ats_core/analysis/cost_analyzer.py

class CostAnalyzer:
    """交易成本分析器"""

    def analyze_execution_costs(self):
        """分析实际执行成本"""
        query = """
        SELECT
            ss.symbol,
            ss.side,
            CAST(json_extract(ss.execution_intent, '$.expected_cost_bps') AS REAL) as expected_cost,
            er.actual_cost_bps,
            json_extract(er.cost_breakdown, '$.spread_bps') as spread,
            json_extract(er.cost_breakdown, '$.slippage_bps') as slippage,
            json_extract(er.cost_breakdown, '$.fee_bps') as fee
        FROM signal_snapshots ss
        JOIN execution_records er ON ss.signal_id = er.signal_id
        """

        results = self.db.query(query)

        # 统计
        total = len(results)

        analysis = {
            "summary": {
                "avg_expected_cost": np.mean([r['expected_cost'] for r in results]),
                "avg_actual_cost": np.mean([r['actual_cost_bps'] for r in results]),
                "avg_spread": np.mean([r['spread'] for r in results]),
                "avg_slippage": np.mean([r['slippage'] for r in results]),
                "avg_fee": np.mean([r['fee'] for r in results])
            },
            "by_symbol": {}
        }

        # 按币种分析
        by_symbol = defaultdict(list)
        for r in results:
            by_symbol[r['symbol']].append(r)

        for symbol, records in by_symbol.items():
            analysis['by_symbol'][symbol] = {
                "count": len(records),
                "avg_actual_cost": np.mean([r['actual_cost_bps'] for r in records]),
                "median_actual_cost": np.median([r['actual_cost_bps'] for r in records]),
                "p95_cost": np.percentile([r['actual_cost_bps'] for r in records], 95)
            }

        return analysis

    def analyze_funding_costs(self):
        """分析资金费率成本"""
        query = """
        SELECT
            ss.symbol,
            tr.hold_duration_hours,
            tr.funding_cost_bps
        FROM signal_snapshots ss
        JOIN trade_results tr ON ss.signal_id = tr.signal_id
        WHERE tr.hold_duration_hours > 0
        """

        results = self.db.query(query)

        # 按持仓时长分析
        duration_buckets = {
            "0-8h": [],
            "8-24h": [],
            "24-48h": [],
            "48h+": []
        }

        for r in results:
            duration = r['hold_duration_hours']
            funding = r['funding_cost_bps']

            if duration < 8:
                duration_buckets["0-8h"].append(funding)
            elif duration < 24:
                duration_buckets["8-24h"].append(funding)
            elif duration < 48:
                duration_buckets["24-48h"].append(funding)
            else:
                duration_buckets["48h+"].append(funding)

        analysis = {}
        for bucket, costs in duration_buckets.items():
            if len(costs) > 0:
                analysis[bucket] = {
                    "count": len(costs),
                    "avg_funding_bps": np.mean(costs),
                    "median_funding_bps": np.median(costs)
                }

        return analysis
```

**4.2 成本模型v2**

```python
# ats_core/features/execution_cost_v2.py

class ExecutionCostEstimatorV2:
    """执行成本估算器 v2 - 基于历史数据"""

    def __init__(self, db_path):
        self.db = Database(db_path)
        self.cost_table = {}  # {symbol: {median_cost, p95_cost}}
        self._load_cost_table()

    def _load_cost_table(self):
        """从历史数据加载成本表"""
        query = """
        SELECT
            ss.symbol,
            er.actual_cost_bps
        FROM signal_snapshots ss
        JOIN execution_records er ON ss.signal_id = er.signal_id
        """

        results = self.db.query(query)

        by_symbol = defaultdict(list)
        for r in results:
            by_symbol[r['symbol']].append(r['actual_cost_bps'])

        for symbol, costs in by_symbol.items():
            if len(costs) >= 10:  # 至少10个样本
                self.cost_table[symbol] = {
                    "median_cost": np.median(costs),
                    "p95_cost": np.percentile(costs, 95),
                    "sample_count": len(costs)
                }

    def estimate_cost(self, symbol, use_conservative=True):
        """估算执行成本"""
        if symbol in self.cost_table:
            # 使用真实数据
            if use_conservative:
                return self.cost_table[symbol]['p95_cost']  # 保守估计（95分位）
            else:
                return self.cost_table[symbol]['median_cost']  # 中位数
        else:
            # Fallback到固定估计
            return 6.0  # 6bps
```

---

### 阶段2实施步骤

#### Week 1-2: 数据采集基础设施
- [ ] 实现TradeRecorder（信号快照、执行记录、交易结果）
- [ ] 创建SQLite数据库schema
- [ ] 集成到现有pipeline（analyze_symbol_v72.py）
- [ ] 部署到生产环境，开始采集数据
- **里程碑**: 第一批信号数据入库

#### Week 3-4: 校准表优化
- [ ] 实现AdaptiveCalibrator
- [ ] 每周更新校准表（当样本数 >= 300）
- [ ] 实现CalibrationMonitor，生成校准曲线
- [ ] 计算Brier Score，评估校准质量
- **里程碑**: 校准表从bootstrap切换到真实数据

#### Week 5-6: 闸门阈值调整
- [ ] 实现GateOptimizer
- [ ] 优化F闸门阈值（扫描-30到+10）
- [ ] 优化市场闸门阈值（独立性阈值）
- [ ] A/B测试：对比优化前后性能
- **里程碑**: 闸门阈值从固定值变为数据驱动

#### Week 7-8: 成本模型精细化
- [ ] 实现CostAnalyzer
- [ ] 分析实际执行成本（spread、slippage、fee）
- [ ] 分析资金费率成本（按持仓时长）
- [ ] 实现ExecutionCostEstimatorV2（基于历史成本）
- **里程碑**: 成本估算从固定6bps变为动态查表

#### Week 9+: 持续监控与迭代
- [ ] 每日监控校准质量（Brier Score）
- [ ] 每周更新闸门阈值（如果数据充足）
- [ ] 每月生成绩效报告（胜率、PnL、Sharpe）
- **里程碑**: 系统进入稳定运行状态

---

### 阶段2成功指标

| 指标 | 初始值（阶段1） | 目标值（阶段2结束） | 衡量方法 |
|------|----------------|---------------------|----------|
| 校准精度（Brier Score） | 未知 | < 0.15 | CalibrationMonitor |
| 胜率预测误差 | ±10% | ±5% | 实际胜率 vs 预测概率 |
| 成本估算误差 | 未知 | ±2bps | 实际成本 vs 预测成本 |
| 信号数据积累 | 0 | 500+ | 数据库记录数 |
| 闸门过滤准确率 | 未知 | > 70% | 被拒绝信号中的坏信号占比 |

---

## 阶段3：ML增强（3-6个月）

### 目标
引入深度学习模型，提升价格预测、因子权重优化、市场状态识别能力。

### 核心任务

#### 1. 数据基础设施升级

**1.1 存储需求**

| 数据类型 | 存储需求 | 保留周期 |
|----------|----------|----------|
| 1m K线（100币种） | ~5GB/年 | 3年（15GB） |
| Orderbook L2（Top 20） | ~50GB/月 | 6月（300GB） |
| Trade flow（逐笔成交） | ~30GB/月 | 6月（180GB） |
| 资金费率历史 | ~100MB/年 | 5年（500MB） |
| OI/CVD数据 | ~2GB/年 | 3年（6GB） |
| 特征工程数据 | ~100GB | 持续 |
| 模型训练数据集 | ~500GB | 持续 |
| 模型检查点 | ~50GB | 持续 |
| **总计** | **~10TB** | |

**1.2 数据采集升级**

```python
# ats_core/data/ml_data_collector.py

class MLDataCollector:
    """ML训练数据采集器"""

    def collect_price_sequence(self, symbol, lookback_hours=168):
        """采集价格序列（用于Transformer训练）"""
        # 1m K线，168h = 7天 = 10080根K线
        klines = self.fetch_klines(symbol, interval='1m', limit=lookback_hours*60)

        features = []
        for k in klines:
            features.append({
                "timestamp": k[0],
                "open": k[1],
                "high": k[2],
                "low": k[3],
                "close": k[4],
                "volume": k[5],

                # 技术指标
                "rsi": self.calc_rsi(klines, current_idx),
                "macd": self.calc_macd(klines, current_idx),
                "bb_upper": self.calc_bb(klines, current_idx)[0],
                "bb_lower": self.calc_bb(klines, current_idx)[1],

                # 高频特征
                "volume_imbalance": self.calc_volume_imbalance(k),
                "price_acceleration": self.calc_price_accel(klines, current_idx)
            })

        return features

    def collect_orderbook_snapshots(self, symbol):
        """采集订单簿快照（用于流动性分析）"""
        # 每5秒采集一次L2 orderbook（bid/ask各20档）
        snapshot = self.fetch_orderbook(symbol, depth=20)

        features = {
            "timestamp": now(),
            "symbol": symbol,
            "bids": snapshot['bids'][:20],  # [[price, qty], ...]
            "asks": snapshot['asks'][:20],

            # 派生特征
            "spread_bps": (snapshot['asks'][0][0] - snapshot['bids'][0][0]) / snapshot['bids'][0][0] * 10000,
            "bid_liquidity_10bps": self._calc_liquidity(snapshot['bids'], bps=10),
            "ask_liquidity_10bps": self._calc_liquidity(snapshot['asks'], bps=10),
            "orderbook_imbalance": self._calc_imbalance(snapshot)
        }

        return features

    def collect_trade_flow(self, symbol):
        """采集逐笔成交（用于资金流分析）"""
        # 采集最近1000笔成交
        trades = self.fetch_recent_trades(symbol, limit=1000)

        # 分类买单/卖单
        buy_volume = sum(t['qty'] for t in trades if t['isBuyerMaker'] == False)
        sell_volume = sum(t['qty'] for t in trades if t['isBuyerMaker'] == True)

        features = {
            "timestamp": now(),
            "symbol": symbol,
            "total_volume": buy_volume + sell_volume,
            "buy_volume": buy_volume,
            "sell_volume": sell_volume,
            "buy_sell_ratio": buy_volume / sell_volume if sell_volume > 0 else 0,

            # 大单统计（>$10k）
            "large_buy_count": sum(1 for t in trades if t['qty'] * t['price'] > 10000 and not t['isBuyerMaker']),
            "large_sell_count": sum(1 for t in trades if t['qty'] * t['price'] > 10000 and t['isBuyerMaker'])
        }

        return features
```

**1.3 特征工程Pipeline**

```python
# ats_core/ml/feature_engineering.py

class FeatureEngineer:
    """特征工程Pipeline"""

    def create_training_dataset(self, symbol, start_date, end_date):
        """创建训练数据集"""
        # 1. 加载原始数据
        klines = self.load_klines(symbol, start_date, end_date, interval='1h')
        oi_data = self.load_oi(symbol, start_date, end_date)
        cvd_data = self.load_cvd(symbol, start_date, end_date)
        funding_data = self.load_funding(symbol, start_date, end_date)

        # 2. 计算v7.2因子
        features = []
        for i in range(168, len(klines)):  # 需要7天历史
            window = klines[i-168:i+1]

            # 原始因子
            T = self.calc_trend(window)
            M = self.calc_momentum(window)
            C = self.calc_capital_flow(window, cvd_data[i])
            V = self.calc_volume(window)
            O = self.calc_oi(window, oi_data[i])
            B = self.calc_basis(window, funding_data[i])
            F = self.calc_fund_leading_v2(window, cvd_data[i-6:i+1], oi_data[i-6:i+1])

            # 市场状态
            I = self.calc_independence(window, btc_klines[i-168:i+1])
            market_regime = self.calc_market_regime(btc_klines[i-168:i+1])

            # 标签（未来1h收益率）
            future_return_1h = (klines[i+1][4] - klines[i][4]) / klines[i][4]

            # 标签（未来4h最大收益/最大回撤）
            future_4h = klines[i+1:i+5]
            max_gain_4h = max((k[2] - klines[i][4]) / klines[i][4] for k in future_4h)
            max_drawdown_4h = min((k[3] - klines[i][4]) / klines[i][4] for k in future_4h)

            features.append({
                # 输入特征
                "T": T, "M": M, "C": C, "V": V, "O": O, "B": B, "F": F,
                "I": I, "market_regime": market_regime,
                "price": klines[i][4],
                "atr": self.calc_atr(window),
                "volatility": self.calc_volatility(window),

                # 标签
                "label_1h_return": future_return_1h,
                "label_4h_max_gain": max_gain_4h,
                "label_4h_max_drawdown": max_drawdown_4h,
                "label_direction": 1 if future_return_1h > 0 else -1
            })

        return pd.DataFrame(features)
```

---

#### 2. Transformer价格预测模型

**2.1 模型架构**

```python
# ats_core/ml/models/price_transformer.py

import torch
import torch.nn as nn

class PriceTransformer(nn.Module):
    """
    Transformer模型预测未来价格

    输入：过去168h的价格序列（1h K线）+ 技术指标
    输出：未来4h的价格区间 [P_min, P_max]
    """

    def __init__(self, input_dim=32, d_model=256, nhead=8, num_layers=6, dropout=0.1):
        super().__init__()

        # 输入嵌入
        self.input_embedding = nn.Linear(input_dim, d_model)

        # 位置编码
        self.pos_encoder = PositionalEncoding(d_model, dropout)

        # Transformer Encoder
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=d_model,
            nhead=nhead,
            dim_feedforward=d_model * 4,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)

        # 输出头
        self.fc_mean = nn.Linear(d_model, 4)  # 预测未来4h的收益率
        self.fc_std = nn.Linear(d_model, 4)   # 预测不确定性

    def forward(self, x):
        """
        x: (batch, seq_len=168, input_dim=32)
        返回: (batch, 4, 2) - 4小时 × [均值, 标准差]
        """
        # 嵌入
        x = self.input_embedding(x)  # (batch, 168, 256)
        x = self.pos_encoder(x)

        # Transformer
        x = self.transformer_encoder(x)  # (batch, 168, 256)

        # 取最后一个时间步
        x = x[:, -1, :]  # (batch, 256)

        # 预测
        mean = self.fc_mean(x)  # (batch, 4)
        std = torch.exp(self.fc_std(x))  # (batch, 4), 保证>0

        return mean, std

class PositionalEncoding(nn.Module):
    """位置编码"""
    def __init__(self, d_model, dropout=0.1, max_len=500):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)

        self.register_buffer('pe', pe)

    def forward(self, x):
        x = x + self.pe[:, :x.size(1), :]
        return self.dropout(x)
```

**2.2 训练Pipeline**

```python
# ats_core/ml/training/train_transformer.py

class TransformerTrainer:
    """Transformer训练器"""

    def __init__(self, model, device='cuda'):
        self.model = model.to(device)
        self.device = device
        self.optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=100)

    def train_epoch(self, dataloader):
        """训练一个epoch"""
        self.model.train()
        total_loss = 0

        for batch in dataloader:
            # batch: {x: (batch, 168, 32), y: (batch, 4)}
            x = batch['x'].to(self.device)
            y_true = batch['y'].to(self.device)

            # 前向传播
            mean, std = self.model(x)

            # 损失函数：负对数似然（假设高斯分布）
            loss = self._nll_loss(mean, std, y_true)

            # 反向传播
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()

            total_loss += loss.item()

        self.scheduler.step()
        return total_loss / len(dataloader)

    def _nll_loss(self, mean, std, y_true):
        """负对数似然损失"""
        # NLL = 0.5 * log(2π) + log(σ) + (y - μ)² / (2σ²)
        nll = 0.5 * torch.log(2 * torch.tensor(math.pi)) + torch.log(std) + (y_true - mean)**2 / (2 * std**2)
        return nll.mean()

    def evaluate(self, dataloader):
        """评估模型"""
        self.model.eval()
        predictions = []
        actuals = []

        with torch.no_grad():
            for batch in dataloader:
                x = batch['x'].to(self.device)
                y_true = batch['y'].cpu().numpy()

                mean, std = self.model(x)
                mean = mean.cpu().numpy()

                predictions.append(mean)
                actuals.append(y_true)

        predictions = np.concatenate(predictions)
        actuals = np.concatenate(actuals)

        # 计算指标
        mae = np.abs(predictions - actuals).mean()
        rmse = np.sqrt(((predictions - actuals)**2).mean())

        # 方向准确率（未来4h是否上涨）
        direction_acc = (np.sign(predictions[:, 3]) == np.sign(actuals[:, 3])).mean()

        return {
            "mae": mae,
            "rmse": rmse,
            "direction_accuracy": direction_acc
        }
```

**2.3 集成到Pipeline**

```python
# ats_core/pipeline/analyze_symbol_v73.py

class V73Analyzer:
    """v7.3分析器 - 集成Transformer预测"""

    def __init__(self):
        self.transformer = self._load_transformer_model()
        self.v72_analyzer = V72Analyzer()

    def analyze(self, symbol, klines, oi_data, cvd_series):
        """分析 + Transformer预测"""
        # 1. v7.2分析（因子 + 闸门 + 校准）
        v72_result = self.v72_analyzer.analyze(symbol, klines, oi_data, cvd_series)

        # 2. Transformer价格预测
        features = self._prepare_transformer_input(klines, oi_data, cvd_series)
        mean_return, std_return = self.transformer.predict(features)

        # mean_return: [1h, 2h, 3h, 4h] 预测收益率
        # std_return: [1h, 2h, 3h, 4h] 预测不确定性

        # 3. 信号融合
        # v7.2判定 + Transformer确认
        v72_signal = v72_result['is_prime_v72']
        v72_side = v72_result['side']

        # Transformer判定：4h预测收益率 > 1.5%
        transformer_bullish = (mean_return[3] > 0.015)
        transformer_bearish = (mean_return[3] < -0.015)

        # 融合规则：两者一致才发信号
        if v72_signal and v72_side == 'long' and transformer_bullish:
            final_signal = 'LONG'
            confidence_boost = 1.2  # Transformer确认，提升信心
        elif v72_signal and v72_side == 'short' and transformer_bearish:
            final_signal = 'SHORT'
            confidence_boost = 1.2
        else:
            final_signal = None
            confidence_boost = 1.0

        # 4. 动态TP/SL
        # 根据Transformer预测的波动率调整
        predicted_4h_range = 2 * std_return[3]  # 95%置信区间
        dynamic_TP = max(0.025, predicted_4h_range * 0.7)  # TP = 70%预测区间
        dynamic_SL = max(0.015, predicted_4h_range * 0.4)  # SL = 40%预测区间

        result = {
            **v72_result,
            "transformer_prediction": {
                "mean_return_4h": mean_return[3],
                "std_return_4h": std_return[3],
                "predicted_range": predicted_4h_range
            },
            "final_signal": final_signal,
            "confidence_boost": confidence_boost,
            "dynamic_TP": dynamic_TP,
            "dynamic_SL": dynamic_SL
        }

        return result
```

---

#### 3. XGBoost因子权重优化

**3.1 模型设计**

```python
# ats_core/ml/models/factor_optimizer.py

import xgboost as xgb

class FactorWeightOptimizer:
    """XGBoost优化因子权重"""

    def __init__(self):
        self.model = None

    def train(self, training_data):
        """
        训练XGBoost模型

        输入特征：T, M, C, V, O, B, F, I, market_regime, atr, volatility
        输出标签：未来4h是否盈利（二分类）
        """
        X = training_data[['T', 'M', 'C', 'V', 'O', 'B', 'F', 'I', 'market_regime', 'atr', 'volatility']]
        y = (training_data['label_4h_max_gain'] > 0.02).astype(int)  # TP=2%

        # 样本权重：近期数据权重更高
        sample_weights = np.exp(np.linspace(-1, 0, len(X)))  # 指数衰减

        # XGBoost参数
        params = {
            'objective': 'binary:logistic',
            'max_depth': 6,
            'learning_rate': 0.05,
            'n_estimators': 500,
            'subsample': 0.8,
            'colsample_bytree': 0.8,
            'min_child_weight': 5,
            'gamma': 0.1,
            'reg_alpha': 0.1,
            'reg_lambda': 1.0,
            'eval_metric': 'auc'
        }

        self.model = xgb.XGBClassifier(**params)
        self.model.fit(X, y, sample_weight=sample_weights)

        # 特征重要性
        feature_importance = self.model.feature_importances_
        importance_dict = dict(zip(X.columns, feature_importance))

        return importance_dict

    def predict_probability(self, factors):
        """预测盈利概率"""
        X = pd.DataFrame([factors])
        prob = self.model.predict_proba(X)[0, 1]  # P(盈利)
        return prob

    def get_optimal_weights(self):
        """从特征重要性推导因子权重"""
        importance = self.model.feature_importances_

        # 归一化到100%
        factor_importance = importance[:7]  # T, M, C, V, O, B, F
        factor_weights = factor_importance / factor_importance.sum()

        return {
            "T": factor_weights[0],
            "M": factor_weights[1],
            "C": factor_weights[2],
            "V": factor_weights[3],
            "O": factor_weights[4],
            "B": factor_weights[5],
            "F": factor_weights[6]
        }
```

**3.2 在线学习**

```python
# ats_core/ml/online_learning.py

class OnlineLearner:
    """在线学习系统 - 持续更新模型"""

    def __init__(self, update_frequency_days=7):
        self.optimizer = FactorWeightOptimizer()
        self.update_frequency = update_frequency_days
        self.last_update = None

    def should_update(self):
        """是否需要更新模型"""
        if self.last_update is None:
            return True

        days_since_update = (now() - self.last_update) / 86400
        return days_since_update >= self.update_frequency

    def update_model(self):
        """更新模型（每周一次）"""
        # 1. 从数据库加载最近3个月的数据
        training_data = self._load_recent_data(months=3)

        # 2. 重新训练XGBoost
        importance = self.optimizer.train(training_data)

        # 3. 获取新的因子权重
        new_weights = self.optimizer.get_optimal_weights()

        # 4. 保存到配置文件
        self._save_weights(new_weights)

        # 5. 记录更新时间
        self.last_update = now()

        print(f"模型已更新，新权重：{new_weights}")
        return new_weights
```

---

#### 4. LSTM市场regime识别

**4.1 模型架构**

```python
# ats_core/ml/models/regime_detector.py

import torch
import torch.nn as nn

class MarketRegimeDetector(nn.Module):
    """
    LSTM识别市场regime

    Regime定义：
    - 0: 震荡（低波动，无明显趋势）
    - 1: 牛市（上升趋势，高成交量）
    - 2: 熊市（下降趋势，恐慌情绪）
    - 3: 高波动（剧烈震荡，方向不明）
    """

    def __init__(self, input_dim=16, hidden_dim=128, num_layers=3, num_regimes=4):
        super().__init__()

        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=0.3
        )

        self.fc = nn.Linear(hidden_dim, num_regimes)
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        """
        x: (batch, seq_len=48, input_dim=16)  # 48h历史
        返回: (batch, 4) - 4个regime的概率分布
        """
        lstm_out, _ = self.lstm(x)  # (batch, 48, 128)
        last_hidden = lstm_out[:, -1, :]  # (batch, 128)

        logits = self.fc(last_hidden)  # (batch, 4)
        probs = self.softmax(logits)

        return probs
```

**4.2 Regime自适应策略**

```python
# ats_core/strategy/regime_adaptive.py

class RegimeAdaptiveStrategy:
    """根据market regime调整策略参数"""

    def __init__(self):
        self.regime_detector = self._load_regime_model()

        # 每个regime的策略参数
        self.regime_configs = {
            0: {  # 震荡市
                "min_confidence": 65,
                "TP_multiplier": 0.8,
                "SL_multiplier": 1.0,
                "position_size": 1.0
            },
            1: {  # 牛市
                "min_confidence": 55,
                "TP_multiplier": 1.5,
                "SL_multiplier": 0.8,
                "position_size": 1.2
            },
            2: {  # 熊市
                "min_confidence": 70,
                "TP_multiplier": 1.0,
                "SL_multiplier": 1.2,
                "position_size": 0.8
            },
            3: {  # 高波动
                "min_confidence": 75,
                "TP_multiplier": 2.0,
                "SL_multiplier": 1.5,
                "position_size": 0.6
            }
        }

    def detect_regime(self, market_features):
        """检测当前market regime"""
        probs = self.regime_detector.predict(market_features)
        regime = np.argmax(probs)
        confidence = probs[regime]

        return regime, confidence

    def adjust_parameters(self, base_signal, regime):
        """根据regime调整信号参数"""
        config = self.regime_configs[regime]

        adjusted_signal = {
            **base_signal,
            "min_confidence": config["min_confidence"],
            "TP": base_signal["TP"] * config["TP_multiplier"],
            "SL": base_signal["SL"] * config["SL_multiplier"],
            "position_size": base_signal["position_size"] * config["position_size"]
        }

        return adjusted_signal
```

---

### 阶段3实施步骤

#### Month 1-2: 数据基础设施
- [ ] 升级存储（租用10TB VPS或对象存储）
- [ ] 部署GPU服务器（租用AWS/GCP GPU实例）
- [ ] 实现MLDataCollector（1m K线、orderbook、trade flow）
- [ ] 实现FeatureEngineer（特征工程pipeline）
- [ ] 创建训练数据集（至少6个月历史）
- **里程碑**: 10TB数据存储就绪，训练数据集准备完毕

#### Month 3-4: Transformer模型训练
- [ ] 实现PriceTransformer架构
- [ ] 训练模型（100 epochs，~1周GPU时间）
- [ ] 超参数调优（学习率、层数、注意力头数）
- [ ] 回测评估（方向准确率 > 60%）
- [ ] 部署推理服务
- **里程碑**: Transformer模型上线，方向准确率达标

#### Month 4-5: XGBoost因子优化
- [ ] 实现FactorWeightOptimizer
- [ ] 训练XGBoost模型（因子 → 盈利概率）
- [ ] 分析特征重要性，推导最优权重
- [ ] 实现在线学习（每周更新）
- [ ] A/B测试：ML权重 vs 固定权重
- **里程碑**: 因子权重从固定变为ML动态优化

#### Month 5-6: LSTM Regime检测
- [ ] 实现MarketRegimeDetector
- [ ] 标注历史数据（手动或半监督）
- [ ] 训练LSTM模型（识别4种regime）
- [ ] 实现RegimeAdaptiveStrategy
- [ ] 回测：Regime自适应 vs 固定策略
- **里程碑**: Regime识别准确率 > 70%，自适应策略上线

#### Month 6+: 集成与优化
- [ ] 集成Transformer + XGBoost + LSTM（v7.3完整版）
- [ ] 端到端回测（6个月历史数据）
- [ ] 模型蒸馏（压缩为小模型，降低延迟）
- [ ] 部署到生产环境（GPU推理）
- **里程碑**: v7.3完整系统上线

---

### 阶段3成功指标

| 指标 | 目标值（阶段3结束） | 衡量方法 |
|------|---------------------|----------|
| Transformer方向准确率 | > 60% | 回测4h方向预测 |
| XGBoost因子权重优化 | 胜率提升+3% | A/B测试 |
| LSTM Regime识别准确率 | > 70% | 标注测试集 |
| 整体Sharpe Ratio | > 2.0 | 6个月回测 |
| 年化收益率 | > 50% | 回测 |
| 最大回撤 | < 15% | 回测 |

---

## 数据架构

### 阶段2数据架构（< 10GB）

```
/home/user/cryptosignal/
├── data/
│   ├── signals.db              # SQLite（信号快照+执行+结果）
│   ├── klines/                 # 1h K线（压缩）
│   ├── oi/                     # OI历史
│   ├── cvd/                    # CVD历史
│   └── funding/                # 资金费率历史
├── models/
│   └── calibration_table.json  # 校准表
└── logs/
    ├── daily_performance.json  # 每日绩效
    └── gate_stats.json         # 闸门统计
```

### 阶段3数据架构（~10TB）

```
/data/
├── raw/                        # 原始数据（8TB）
│   ├── klines_1m/              # 1分钟K线（3年×100币种）
│   ├── orderbook/              # L2订单簿快照（6月×20币种）
│   ├── trades/                 # 逐笔成交（6月×100币种）
│   ├── funding/                # 资金费率（5年）
│   └── oi_cvd/                 # OI+CVD（3年）
├── features/                   # 特征工程（1TB）
│   ├── technical_indicators/   # 技术指标
│   ├── orderbook_features/     # 订单簿特征
│   └── regime_labels/          # Regime标注
├── datasets/                   # 训练集（500GB）
│   ├── transformer/            # Transformer训练集
│   ├── xgboost/                # XGBoost训练集
│   └── lstm/                   # LSTM训练集
└── models/                     # 模型（50GB）
    ├── transformer/
    │   ├── checkpoints/        # 检查点
    │   └── best_model.pth
    ├── xgboost/
    │   └── factor_optimizer.model
    └── lstm/
        └── regime_detector.pth
```

---

## 实施时间线

### 阶段2时间线（8周）

```
Week 1-2: 数据采集基础设施
├── W1: 实现TradeRecorder + SQLite schema
└── W2: 集成到pipeline，开始采集数据

Week 3-4: 校准表优化
├── W3: 实现AdaptiveCalibrator
└── W4: CalibrationMonitor + Brier Score

Week 5-6: 闸门阈值调整
├── W5: 实现GateOptimizer
└── W6: A/B测试，应用新阈值

Week 7-8: 成本模型精细化
├── W7: 实现CostAnalyzer
└── W8: ExecutionCostEstimatorV2，持续监控
```

### 阶段3时间线（6个月）

```
Month 1-2: 数据基础设施
├── M1: 升级存储（10TB）+ GPU服务器
└── M2: 数据采集 + 特征工程

Month 3-4: Transformer + XGBoost
├── M3: Transformer训练（1周GPU）+ 回测
└── M4: XGBoost训练 + 在线学习

Month 5-6: LSTM + 集成
├── M5: LSTM训练 + Regime自适应
└── M6: 端到端集成 + 生产部署
```

---

## 成功指标

### 阶段2成功标准

| 维度 | 指标 | 目标 | 当前 |
|------|------|------|------|
| **校准精度** | Brier Score | < 0.15 | TBD |
| **胜率预测** | 误差 | ±5% | ±10% |
| **成本估算** | 误差 | ±2bps | 未知 |
| **数据积累** | 样本数 | 500+ | 0 |
| **系统稳定性** | 正常运行时间 | > 99% | TBD |

### 阶段3成功标准

| 维度 | 指标 | 目标 | 当前 |
|------|------|------|------|
| **价格预测** | 方向准确率 | > 60% | TBD |
| **因子优化** | 胜率提升 | +3% | 0% |
| **Regime识别** | 准确率 | > 70% | TBD |
| **整体绩效** | Sharpe Ratio | > 2.0 | TBD |
| **风险控制** | 最大回撤 | < 15% | TBD |

---

## 风险评估

### 阶段2风险

| 风险 | 概率 | 影响 | 缓解措施 |
|------|------|------|----------|
| **数据采集失败** | 中 | 高 | 多重数据源，备用采集器 |
| **样本数不足** | 中 | 中 | 降低最小样本要求，延长采集周期 |
| **校准表过拟合** | 低 | 中 | 交叉验证，分桶最小样本数 |
| **闸门过于严格** | 中 | 中 | A/B测试，逐步调整 |

### 阶段3风险

| 风险 | 概率 | 影响 | 缓解措施 |
|------|------|------|----------|
| **GPU资源不足** | 中 | 高 | 租用云GPU（AWS/GCP），分批训练 |
| **存储成本超预算** | 中 | 中 | 对象存储（S3），数据压缩 |
| **模型过拟合** | 高 | 高 | 正则化，Dropout，早停 |
| **推理延迟过高** | 中 | 中 | 模型蒸馏，量化，异步推理 |
| **ML模型失效** | 低 | 高 | 持续监控，自动回退到v7.2 |

---

## 成本预算

### 阶段2成本（~$50/月）

| 项目 | 规格 | 成本 |
|------|------|------|
| VPS | 50GB存储，4核CPU | $30/月 |
| 备份存储 | S3 10GB | $5/月 |
| 监控服务 | Grafana Cloud免费版 | $0 |
| **总计** | | **$35/月** |

### 阶段3成本（~$500-800/月）

| 项目 | 规格 | 成本 |
|------|------|------|
| GPU服务器 | AWS p3.2xlarge (1× V100) | $3/小时 × 100h = $300/月 |
| 存储 | S3 10TB | $230/月 |
| CPU服务器 | 推理服务（16核，64GB） | $150/月 |
| 带宽 | 100GB/月 | $10/月 |
| **总计** | | **$690/月** |

**成本优化**：
- 训练阶段：使用Spot实例（节省70%）
- 推理阶段：模型蒸馏为小模型，CPU推理
- 存储：冷数据归档到Glacier（节省80%）

---

## 总结

### 阶段2关键点
1. **数据驱动**：从规则系统进化为统计系统
2. **持续优化**：校准表、闸门阈值、成本模型每周更新
3. **低成本**：< 10GB存储，$35/月，无需GPU
4. **8周交付**：快速迭代，尽早看到效果

### 阶段3关键点
1. **ML增强**：Transformer价格预测、XGBoost因子优化、LSTM Regime识别
2. **高成本**：10TB存储，GPU训练，$690/月
3. **6个月交付**：需要充足的训练数据和调优时间
4. **高回报**：预期Sharpe > 2.0，年化收益 > 50%

**建议**：
- 先全力做好阶段2（2个月），验证数据采集和统计优化可行性
- 阶段2稳定运行后，评估是否有必要进入阶段3
- 如果阶段2已经达到满意效果（Sharpe > 1.5），可能不需要阶段3的复杂性
