import os, time, json
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from ats_core.cfg import CFG
from ats_core.pools.pool_manager import get_pool_manager
from ats_core.pipeline.analyze_symbol import analyze_symbol
from ats_core.outputs.telegram_fmt import render_trade, render_watch
from ats_core.outputs.publisher import telegram_send
from ats_core.logging import log, warn
from ats_core.utils.rate_limiter import SAFE_LIMITER

DATA = os.path.join(os.path.dirname(os.path.dirname(__file__)), "..", "data", "reports")
os.makedirs(DATA, exist_ok=True)

def batch_run():
    """
    æ‰¹é‡æ‰«æï¼ˆä¼˜åŒ–ç‰ˆ - ä½¿ç”¨æ™ºèƒ½ç¼“å­˜æ± ï¼‰

    æ–°æ¶æ„:
    - Elite Pool (24hç¼“å­˜): ç¨³å®šå¸ç§
    - Overlay Pool (1hç¼“å­˜): å¼‚å¸¸å¸ç§ + æ–°å¸
    - APIè°ƒç”¨é‡: -90%
    - æ‰«æé€Ÿåº¦: +10å€
    """
    # ä½¿ç”¨æ–°çš„æ± ç®¡ç†å™¨ï¼ˆè‡ªåŠ¨å¤„ç†ç¼“å­˜ï¼‰
    manager = get_pool_manager(
        elite_cache_hours=24,
        overlay_cache_hours=1,
        verbose=True
    )

    # è·å–åˆå¹¶åçš„å€™é€‰æ± ï¼ˆè‡ªåŠ¨æ£€æŸ¥ç¼“å­˜æœ‰æ•ˆæœŸï¼‰
    syms, metadata = manager.get_merged_universe()

    log(f"ğŸš€ å¼€å§‹æ‰¹é‡æ‰«æ: {len(syms)} ä¸ªå¸ç§")
    log(f"   Elite Pool: {metadata['elite_count']} ä¸ª (ç¼“å­˜{'æœ‰æ•ˆ' if metadata['elite_cache_valid'] else 'é‡å»º'})")
    log(f"   Overlay Pool: {metadata['overlay_count']} ä¸ª (ç¼“å­˜{'æœ‰æ•ˆ' if metadata['overlay_cache_valid'] else 'é‡å»º'})")
    log(f"   APIä¼˜åŒ–: ~90% è°ƒç”¨é‡é™ä½ ğŸš€")
    for sym in syms:
        try:
            r = analyze_symbol(sym)
            pub = r.get("publish") or {}
            html = render_trade(r) if pub.get("prime") else render_watch(r)
            telegram_send(html)
            # save report
            ts=time.strftime("%Y%m%dT%H%MZ", time.gmtime())
            with open(os.path.join(DATA, f"scan_{sym}_{ts}.md"),"w",encoding="utf-8") as f:
                f.write(html)
        except Exception as e:
            warn("batch %s error: %s", sym, e)
        time.sleep(CFG.get("limits","per_symbol_delay_ms", default=600)/1000.0)


def batch_run_parallel(max_workers: int = 5, use_v2: bool = False, v2_config: str = None) -> Dict[str, Any]:
    """
    å¹¶è¡Œæ‰¹é‡æ‰«æï¼ˆå¸¦APIé™æµä¿æŠ¤ï¼‰

    Args:
        max_workers: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤5ï¼Œä¿å®ˆé…ç½®é˜²æ­¢é£æ§ï¼‰
        use_v2: æ˜¯å¦ä½¿ç”¨v2åˆ†æå™¨ï¼ˆé»˜è®¤Falseï¼‰
        v2_config: V2é…ç½®æ–‡ä»¶åï¼ˆé»˜è®¤Noneï¼Œä½¿ç”¨factors_unified.jsonï¼‰
                  å¯é€‰: "factors_v2_lite.json"ï¼ˆ8ç»´è½»é‡ç‰ˆï¼‰

    Returns:
        æ‰«æç»Ÿè®¡ä¿¡æ¯

    ç‰¹æ€§:
    - ThreadPoolExecutorå¹¶å‘æ‰§è¡Œ
    - SafeRateLimiteré˜²æ­¢APIé£æ§ï¼ˆ60req/minï¼‰
    - è‡ªåŠ¨é”™è¯¯æ¢å¤
    - å®æ—¶è¿›åº¦æ˜¾ç¤º
    - æ”¯æŒV2 Liteè½»é‡ç‰ˆï¼ˆ8+1ç»´ï¼Œæ— éœ€è®¢å•ç°¿/æ¸…ç®—æ•°æ®ï¼‰
    """
    from ats_core.pipeline.analyze_symbol_v2 import analyze_symbol_v2

    # ä½¿ç”¨æ™ºèƒ½æ± ç®¡ç†å™¨
    manager = get_pool_manager(
        elite_cache_hours=24,
        overlay_cache_hours=1,
        verbose=True
    )

    # è·å–å€™é€‰æ± 
    syms, metadata = manager.get_merged_universe()

    # é¡¹ç›®æ ¹ç›®å½•ï¼ˆç”¨äºæ„å»ºé…ç½®æ–‡ä»¶è·¯å¾„ï¼‰
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    log(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰¹é‡æ‰«æ: {len(syms)} ä¸ªå¸ç§")
    log(f"   å¹¶å‘æ•°: {max_workers} (ä¿å®ˆé…ç½®ï¼Œé˜²é£æ§)")
    log(f"   é™æµç­–ç•¥: {SAFE_LIMITER.requests_per_minute} req/min")
    log(f"   Elite Pool: {metadata['elite_count']} ä¸ª")
    log(f"   Overlay Pool: {metadata['overlay_count']} ä¸ª")

    # åˆ†æå‡½æ•°é€‰æ‹©
    if use_v2:
        if v2_config:
            log(f"   åˆ†æå™¨: V2 ({v2_config})")
            config_path = os.path.join(project_root, "config", v2_config)
            analyze_func = lambda sym: analyze_symbol_v2(sym, config_path=config_path)
        else:
            log(f"   åˆ†æå™¨: V2 (é»˜è®¤é…ç½®)")
            analyze_func = analyze_symbol_v2
    else:
        log(f"   åˆ†æå™¨: V1 (ç”Ÿäº§ç‰ˆ)")
        analyze_func = analyze_symbol

    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(syms),
        'completed': 0,
        'prime_signals': 0,
        'watch_signals': 0,
        'errors': 0,
        'start_time': time.time()
    }

    def process_symbol(symbol: str) -> Optional[Dict]:
        """å¤„ç†å•ä¸ªsymbolï¼ˆå¸¦é™æµï¼‰"""
        try:
            # ä½¿ç”¨é™æµå™¨åŒ…è£…
            def analyze_with_limit():
                return analyze_func(symbol)

            # execute_safeä¼šè‡ªåŠ¨å¤„ç†é™æµ
            result = analyze_with_limit()

            return {
                'symbol': symbol,
                'result': result,
                'success': True
            }

        except Exception as e:
            warn(f"[ParallelScan] {symbol} åˆ†æå¤±è´¥: {e}")
            return {
                'symbol': symbol,
                'error': str(e),
                'success': False
            }

    # åˆ›å»ºä»»åŠ¡åˆ—è¡¨ï¼ˆlambdaåŒ…è£…ï¼Œå»¶è¿Ÿæ‰§è¡Œï¼‰
    tasks = [lambda s=sym: process_symbol(s) for sym in syms]

    # ä½¿ç”¨SafeRateLimiterçš„execute_safeæ–¹æ³•ï¼ˆè‡ªå¸¦é™æµ+å¹¶å‘æ§åˆ¶ï¼‰
    log(f"\nå¼€å§‹å¹¶è¡Œæ‰«æ...")
    results = SAFE_LIMITER.execute_safe(
        tasks=tasks,
        task_names=[f"åˆ†æ {sym}" for sym in syms],
        show_progress=True
    )

    # å¤„ç†ç»“æœ
    log(f"\nå¤„ç†æ‰«æç»“æœ...")

    for result in results:
        if result is None or not isinstance(result, dict):
            stats['errors'] += 1
            continue

        stats['completed'] += 1

        if not result.get('success'):
            stats['errors'] += 1
            continue

        # æå–åˆ†æç»“æœ
        symbol = result['symbol']
        analysis = result['result']

        # åˆ¤æ–­ä¿¡å·ç±»å‹
        pub = analysis.get("publish") or {}

        try:
            # æ¸²æŸ“HTML
            if pub.get("prime"):
                html = render_trade(analysis)
                stats['prime_signals'] += 1
            elif pub.get("watch"):
                html = render_watch(analysis)
                stats['watch_signals'] += 1
            else:
                continue  # ä¸å‘å¸ƒ

            # å‘é€Telegram
            telegram_send(html)

            # ä¿å­˜æŠ¥å‘Š
            ts = time.strftime("%Y%m%dT%H%MZ", time.gmtime())
            report_path = os.path.join(DATA, f"scan_{symbol}_{ts}.md")
            with open(report_path, "w", encoding="utf-8") as f:
                f.write(html)

        except Exception as e:
            warn(f"[ParallelScan] {symbol} è¾“å‡ºå¤±è´¥: {e}")
            stats['errors'] += 1

    # è®¡ç®—ç»Ÿè®¡
    elapsed = time.time() - stats['start_time']
    stats['elapsed_seconds'] = round(elapsed, 2)
    stats['symbols_per_second'] = round(stats['completed'] / elapsed, 2) if elapsed > 0 else 0

    # æ‰“å°æ±‡æ€»
    log(f"\n{'='*60}")
    log(f"ğŸ“Š æ‰¹é‡æ‰«æå®Œæˆ")
    log(f"{'='*60}")
    log(f"  æ€»æ•°: {stats['total']}")
    log(f"  æˆåŠŸ: {stats['completed']}")
    log(f"  é”™è¯¯: {stats['errors']}")
    log(f"  Primeä¿¡å·: {stats['prime_signals']}")
    log(f"  Watchä¿¡å·: {stats['watch_signals']}")
    log(f"  è€—æ—¶: {stats['elapsed_seconds']}ç§’")
    log(f"  é€Ÿåº¦: {stats['symbols_per_second']} symbols/s")
    log(f"  APIé™æµç»Ÿè®¡: {SAFE_LIMITER.get_stats() if hasattr(SAFE_LIMITER, 'get_stats') else 'N/A'}")
    log(f"{'='*60}")

    return stats