# coding: utf-8
"""
å¼ºåŒ–å­¦ä¹ åŠ¨æ€æ­¢æŸç³»ç»Ÿï¼ˆDQNæ¡†æ¶ï¼‰

ç†è®ºåŸºç¡€ï¼šDeep Q-Network (DQN)
- State: [profit%, hold_time, volatility, signal_prob, market_regime]
- Action: [ä¿æŒæ­¢æŸ, ç§»è‡³breakeven, æ”¶ç´§10%, æ”¾å®½10%]
- Reward: æœ€ç»ˆç›ˆäº + é£é™©è°ƒæ•´

è®­ç»ƒæµç¨‹ï¼š
1. æ”¶é›†å†å²äº¤æ˜“æ•°æ®
2. è®­ç»ƒDQNæ¨¡å‹
3. å›æµ‹éªŒè¯
4. å®ç›˜éƒ¨ç½²

ä½¿ç”¨æ–¹æ³•ï¼š
    from ats_core.rl.dynamic_stop_loss import DynamicStopLossAgent

    agent = DynamicStopLossAgent()
    agent.load_model("models/dqn_stop_loss_v1.pth")

    # åœ¨äº¤æ˜“ä¸­ä½¿ç”¨
    action = agent.get_action(state)
    new_stop_loss = agent.apply_action(current_stop_loss, action)

æ³¨æ„ï¼š
- éœ€è¦å¤§é‡å†å²äº¤æ˜“æ•°æ®è®­ç»ƒ
- å»ºè®®å…ˆç”¨æ¨¡æ‹Ÿæ•°æ®è®­ç»ƒ
- å®ç›˜å‰å……åˆ†å›æµ‹
"""

from __future__ import annotations
import numpy as np
from typing import List, Tuple, Dict, Any, Optional
from collections import deque
import random


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒº"""

    def __init__(self, capacity: int = 10000):
        self.buffer = deque(maxlen=capacity)

    def push(self, state, action, reward, next_state, done):
        """æ·»åŠ ç»éªŒ"""
        self.buffer.append((state, action, reward, next_state, done))

    def sample(self, batch_size: int) -> List:
        """éšæœºé‡‡æ ·"""
        return random.sample(self.buffer, min(batch_size, len(self.buffer)))

    def __len__(self):
        return len(self.buffer)


class DynamicStopLossAgent:
    """
    åŠ¨æ€æ­¢æŸDQNæ™ºèƒ½ä½“

    State (5ç»´):
        - profit_pct: å½“å‰ç›ˆäºç™¾åˆ†æ¯” (-10% to +50%)
        - hold_time_hours: æŒä»“æ—¶é—´ï¼ˆå°æ—¶ï¼‰(0-72h)
        - volatility: å½“å‰æ³¢åŠ¨ç‡ (0-5%)
        - signal_probability: ä¿¡å·æ¦‚ç‡ (0-1)
        - market_regime: å¸‚åœºä½“åˆ¶ (-100 to +100)

    Action (4ä¸ª):
        0: ä¿æŒå½“å‰æ­¢æŸ
        1: ç§»è‡³breakevenï¼ˆç›ˆäºå¹³è¡¡ç‚¹ï¼‰
        2: æ”¶ç´§10%ï¼ˆå‘entryé è¿‘ï¼‰
        3: æ”¾å®½10%ï¼ˆè¿œç¦»entryï¼‰

    Reward:
        - è§¦å‘æ­¢æŸ: -abs(loss_pct) * 100
        - æ­¢ç›ˆå‡ºåœº: +profit_pct * 100
        - æŒä»“ä¸­: -0.1 * hold_time (æŒä»“æˆæœ¬)
    """

    def __init__(
        self,
        state_dim: int = 5,
        action_dim: int = 4,
        learning_rate: float = 0.001,
        gamma: float = 0.99,
        epsilon: float = 1.0,
        epsilon_decay: float = 0.995,
        epsilon_min: float = 0.01
    ):
        """
        Args:
            state_dim: çŠ¶æ€ç»´åº¦
            action_dim: åŠ¨ä½œç»´åº¦
            learning_rate: å­¦ä¹ ç‡
            gamma: æŠ˜æ‰£å› å­
            epsilon: æ¢ç´¢ç‡
            epsilon_decay: æ¢ç´¢ç‡è¡°å‡
            epsilon_min: æœ€å°æ¢ç´¢ç‡
        """
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.learning_rate = learning_rate
        self.gamma = gamma
        self.epsilon = epsilon
        self.epsilon_decay = epsilon_decay
        self.epsilon_min = epsilon_min

        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(capacity=10000)

        # Qç½‘ç»œï¼ˆéœ€è¦PyTorchå®ç°ï¼Œè¿™é‡Œæä¾›æ¥å£ï¼‰
        self.q_network = None  # TODO: å®ç°ç¥ç»ç½‘ç»œ
        self.target_network = None  # TODO: å®ç°ç›®æ ‡ç½‘ç»œ

        # ç»Ÿè®¡
        self.training_steps = 0
        self.episode_rewards = []

        print("[DQN] åŠ¨æ€æ­¢æŸæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
        print("[DQN] âš ï¸ æ³¨æ„ï¼šéœ€è¦å®‰è£… PyTorch: pip install torch")
        print("[DQN] âš ï¸ éœ€è¦å†å²äº¤æ˜“æ•°æ®è¿›è¡Œè®­ç»ƒ")

    def get_action(self, state: np.ndarray, explore: bool = True) -> int:
        """
        æ ¹æ®çŠ¶æ€é€‰æ‹©åŠ¨ä½œ

        Args:
            state: çŠ¶æ€å‘é‡ (5ç»´)
            explore: æ˜¯å¦æ¢ç´¢ï¼ˆè®­ç»ƒæ—¶Trueï¼Œæ¨ç†æ—¶Falseï¼‰

        Returns:
            åŠ¨ä½œç´¢å¼• (0-3)
        """
        # Îµ-greedyç­–ç•¥
        if explore and np.random.rand() < self.epsilon:
            return np.random.randint(self.action_dim)

        # Qç½‘ç»œæ¨ç†ï¼ˆTODO: å®ç°ï¼‰
        # q_values = self.q_network(state)
        # return np.argmax(q_values)

        # ä¸´æ—¶ç­–ç•¥ï¼ˆæ¨¡æ‹Ÿï¼‰
        if state[0] > 0.05:  # ç›ˆåˆ©>5%
            return 1  # ç§»è‡³breakeven
        elif state[0] < -0.03:  # äºæŸ>3%
            return 0  # ä¿æŒæ­¢æŸ
        elif state[4] > 60:  # å¼ºåŠ¿å¸‚åœº
            return 3  # æ”¾å®½æ­¢æŸ
        else:
            return 0  # é»˜è®¤ä¿æŒ

    def apply_action(
        self,
        entry_price: float,
        current_price: float,
        current_stop_loss: float,
        action: int,
        direction: str = "LONG"
    ) -> float:
        """
        åº”ç”¨åŠ¨ä½œï¼Œè®¡ç®—æ–°çš„æ­¢æŸä»·æ ¼

        Args:
            entry_price: å…¥åœºä»·æ ¼
            current_price: å½“å‰ä»·æ ¼
            current_stop_loss: å½“å‰æ­¢æŸä»·
            action: åŠ¨ä½œç´¢å¼•
            direction: æ–¹å‘ï¼ˆLONG/SHORTï¼‰

        Returns:
            æ–°çš„æ­¢æŸä»·æ ¼
        """
        if direction == "LONG":
            if action == 0:
                # ä¿æŒ
                new_stop_loss = current_stop_loss

            elif action == 1:
                # ç§»è‡³breakeven
                new_stop_loss = entry_price * 0.998  # ç•¥ä½äºentryï¼Œè¦†ç›–æ‰‹ç»­è´¹

            elif action == 2:
                # æ”¶ç´§10%
                distance = current_price - current_stop_loss
                new_stop_loss = current_stop_loss + distance * 0.1

            elif action == 3:
                # æ”¾å®½10%
                distance = current_price - current_stop_loss
                new_stop_loss = current_stop_loss - distance * 0.1

            else:
                new_stop_loss = current_stop_loss

            # ç¡®ä¿æ­¢æŸä¸é«˜äºå½“å‰ä»·
            new_stop_loss = min(new_stop_loss, current_price * 0.995)

        else:  # SHORT
            if action == 0:
                new_stop_loss = current_stop_loss

            elif action == 1:
                new_stop_loss = entry_price * 1.002

            elif action == 2:
                distance = current_stop_loss - current_price
                new_stop_loss = current_stop_loss - distance * 0.1

            elif action == 3:
                distance = current_stop_loss - current_price
                new_stop_loss = current_stop_loss + distance * 0.1

            else:
                new_stop_loss = current_stop_loss

            # ç¡®ä¿æ­¢æŸä¸ä½äºå½“å‰ä»·
            new_stop_loss = max(new_stop_loss, current_price * 1.005)

        return new_stop_loss

    def train_step(self, batch_size: int = 64) -> Optional[float]:
        """
        è®­ç»ƒä¸€æ­¥

        Args:
            batch_size: æ‰¹é‡å¤§å°

        Returns:
            æŸå¤±å€¼
        """
        if len(self.replay_buffer) < batch_size:
            return None

        # é‡‡æ ·
        batch = self.replay_buffer.sample(batch_size)

        # TODO: å®ç°DQNè®­ç»ƒé€»è¾‘
        # 1. è§£åŒ…batch
        # 2. è®¡ç®—Qå€¼
        # 3. è®¡ç®—ç›®æ ‡Qå€¼
        # 4. è®¡ç®—loss
        # 5. åå‘ä¼ æ’­
        # 6. æ›´æ–°å‚æ•°

        # æ›´æ–°epsilon
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)

        self.training_steps += 1

        return 0.0  # ä¸´æ—¶è¿”å›

    def train_episode(
        self,
        trade_data: Dict[str, Any],
        verbose: bool = False
    ) -> float:
        """
        è®­ç»ƒä¸€ä¸ªäº¤æ˜“episode

        Args:
            trade_data: äº¤æ˜“æ•°æ®ï¼ŒåŒ…å«:
                - entry_price: å…¥åœºä»·
                - klines: Kçº¿æ•°æ®
                - signal_prob: ä¿¡å·æ¦‚ç‡
                - market_regime: å¸‚åœºä½“åˆ¶
                - direction: æ–¹å‘
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯

        Returns:
            Episodeæ€»å¥–åŠ±
        """
        # TODO: å®ç°å®Œæ•´è®­ç»ƒæµç¨‹
        # 1. åˆå§‹åŒ–çŠ¶æ€
        # 2. å¾ªç¯ç›´åˆ°äº¤æ˜“ç»“æŸ
        # 3. é€‰æ‹©åŠ¨ä½œ
        # 4. æ‰§è¡ŒåŠ¨ä½œ
        # 5. è®¡ç®—å¥–åŠ±
        # 6. å­˜å‚¨ç»éªŒ
        # 7. è®­ç»ƒç½‘ç»œ

        episode_reward = 0.0

        if verbose:
            print(f"[DQN] Episodeå®Œæˆï¼Œå¥–åŠ±: {episode_reward:.2f}")

        self.episode_rewards.append(episode_reward)
        return episode_reward

    def save_model(self, path: str) -> None:
        """ä¿å­˜æ¨¡å‹"""
        print(f"[DQN] ä¿å­˜æ¨¡å‹åˆ° {path}")
        # TODO: å®ç°æ¨¡å‹ä¿å­˜
        # torch.save(self.q_network.state_dict(), path)

    def load_model(self, path: str) -> None:
        """åŠ è½½æ¨¡å‹"""
        print(f"[DQN] ä» {path} åŠ è½½æ¨¡å‹")
        # TODO: å®ç°æ¨¡å‹åŠ è½½
        # self.q_network.load_state_dict(torch.load(path))

    def get_stats(self) -> Dict[str, Any]:
        """è·å–è®­ç»ƒç»Ÿè®¡"""
        return {
            'training_steps': self.training_steps,
            'epsilon': self.epsilon,
            'buffer_size': len(self.replay_buffer),
            'avg_reward_100ep': np.mean(self.episode_rewards[-100:]) if self.episode_rewards else 0.0
        }


# ========== å·¥å…·å‡½æ•° ==========

def build_state(
    entry_price: float,
    current_price: float,
    entry_time: float,
    current_time: float,
    volatility: float,
    signal_probability: float,
    market_regime: float
) -> np.ndarray:
    """
    æ„å»ºçŠ¶æ€å‘é‡

    Returns:
        çŠ¶æ€å‘é‡ (5ç»´)
    """
    # 1. ç›ˆäºç™¾åˆ†æ¯”
    profit_pct = (current_price - entry_price) / entry_price

    # 2. æŒä»“æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    hold_time_hours = (current_time - entry_time) / 3600

    # 3. æ³¢åŠ¨ç‡ (å½’ä¸€åŒ–)
    volatility_norm = volatility  # å‡è®¾å·²å½’ä¸€åŒ–

    # 4. ä¿¡å·æ¦‚ç‡
    signal_prob_norm = signal_probability

    # 5. å¸‚åœºä½“åˆ¶ï¼ˆå½’ä¸€åŒ–åˆ°[-1, 1]ï¼‰
    market_regime_norm = market_regime / 100.0

    state = np.array([
        profit_pct,
        hold_time_hours / 72.0,  # å½’ä¸€åŒ–åˆ°0-1
        volatility_norm,
        signal_prob_norm,
        market_regime_norm
    ], dtype=np.float32)

    return state


# ========== æµ‹è¯•ä»£ç  ==========

if __name__ == "__main__":
    print("=" * 70)
    print("å¼ºåŒ–å­¦ä¹ åŠ¨æ€æ­¢æŸç³»ç»Ÿæµ‹è¯•")
    print("=" * 70)

    # åˆ›å»ºæ™ºèƒ½ä½“
    agent = DynamicStopLossAgent()

    # æµ‹è¯•åœºæ™¯
    print("\n[æµ‹è¯•1] ç›ˆåˆ©5%ï¼Œå¸‚åœºå¼ºåŠ¿")
    state = build_state(
        entry_price=50000,
        current_price=52500,
        entry_time=0,
        current_time=7200,  # 2å°æ—¶
        volatility=0.02,
        signal_probability=0.75,
        market_regime=70
    )
    action = agent.get_action(state, explore=False)
    print(f"  çŠ¶æ€: profit=+5%, hold=2h, vol=2%, prob=75%, regime=+70")
    print(f"  åŠ¨ä½œ: {action} ({'ä¿æŒ/ç§»å¹³/æ”¶ç´§/æ”¾å®½'.split('/')[action]})")

    # åº”ç”¨åŠ¨ä½œ
    new_sl = agent.apply_action(
        entry_price=50000,
        current_price=52500,
        current_stop_loss=49000,
        action=action,
        direction="LONG"
    )
    print(f"  æ—§æ­¢æŸ: 49000, æ–°æ­¢æŸ: {new_sl:.0f}")

    print("\n[æµ‹è¯•2] äºæŸ3%ï¼Œå¸‚åœºéœ‡è¡")
    state = build_state(
        entry_price=50000,
        current_price=48500,
        entry_time=0,
        current_time=3600,  # 1å°æ—¶
        volatility=0.03,
        signal_probability=0.55,
        market_regime=10
    )
    action = agent.get_action(state, explore=False)
    print(f"  çŠ¶æ€: profit=-3%, hold=1h, vol=3%, prob=55%, regime=+10")
    print(f"  åŠ¨ä½œ: {action} ({'ä¿æŒ/ç§»å¹³/æ”¶ç´§/æ”¾å®½'.split('/')[action]})")

    new_sl = agent.apply_action(
        entry_price=50000,
        current_price=48500,
        current_stop_loss=49000,
        action=action,
        direction="LONG"
    )
    print(f"  æ—§æ­¢æŸ: 49000, æ–°æ­¢æŸ: {new_sl:.0f}")

    # ç»Ÿè®¡
    print(f"\nç»Ÿè®¡: {agent.get_stats()}")

    print("\n" + "=" * 70)
    print("âœ… å¼ºåŒ–å­¦ä¹ æ­¢æŸæ¡†æ¶æµ‹è¯•å®Œæˆ")
    print("=" * 70)
    print("\nğŸ“Œ è®­ç»ƒæ­¥éª¤ï¼š")
    print("1. pip install torch numpy")
    print("2. æ”¶é›†å†å²äº¤æ˜“æ•°æ®ï¼ˆè‡³å°‘1000ç¬”ï¼‰")
    print("3. å®ç°Qç½‘ç»œï¼ˆ3å±‚MLP: 5â†’64â†’64â†’4ï¼‰")
    print("4. è®­ç»ƒagentï¼ˆå»ºè®®10000+ episodesï¼‰")
    print("5. å›æµ‹éªŒè¯")
    print("6. å®ç›˜éƒ¨ç½²")
