# coding: utf-8
"""
å›æµ‹æ•°æ®åŠ è½½å™¨

åŠŸèƒ½ï¼š
1. ä»æ•°æ®åº“åŠ è½½å†å²ä¿¡å·
2. ä»å¸å®‰APIè·å–å†å²Kçº¿æ•°æ®
3. ç¼“å­˜æ•°æ®ä»¥æé«˜å›æµ‹é€Ÿåº¦
"""
import os
import sys
import json
from typing import List, Dict, Optional
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from ats_core.sources.binance_safe import get_klines


class BacktestDataLoader:
    """
    å›æµ‹æ•°æ®åŠ è½½å™¨

    æä¾›å›æµ‹æ‰€éœ€çš„æ‰€æœ‰æ•°æ®ï¼š
    - å†å²ä¿¡å·ï¼ˆä»æ•°æ®åº“ï¼‰
    - å†å²ä»·æ ¼ï¼ˆä»APIæˆ–ç¼“å­˜ï¼‰
    """

    def __init__(self, cache_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        Args:
            cache_dir: ç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤data/backtest/cacheï¼‰
        """
        if cache_dir is None:
            project_root = Path(__file__).parent.parent
            cache_dir = project_root / "data" / "backtest" / "cache"

        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def load_signals_from_database(
        self,
        start_time: datetime,
        end_time: datetime,
        symbols: Optional[List[str]] = None,
        min_probability: Optional[float] = None
    ) -> List[Dict]:
        """
        ä»æ•°æ®åº“åŠ è½½å†å²ä¿¡å·

        Args:
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            symbols: å¸ç§è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
            min_probability: æœ€å°æ¦‚ç‡è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¿¡å·åˆ—è¡¨
        """
        try:
            from ats_core.database.models import db, Signal
            from sqlalchemy import and_

            session = db.get_session()

            # æ„å»ºæŸ¥è¯¢
            query = session.query(Signal).filter(
                and_(
                    Signal.timestamp >= start_time,
                    Signal.timestamp <= end_time
                )
            )

            # å¸ç§è¿‡æ»¤
            if symbols:
                query = query.filter(Signal.symbol.in_(symbols))

            # æ¦‚ç‡è¿‡æ»¤
            if min_probability:
                query = query.filter(Signal.probability >= min_probability)

            # æ‰§è¡ŒæŸ¥è¯¢
            db_signals = query.order_by(Signal.timestamp).all()

            # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            signals = []
            for s in db_signals:
                signals.append({
                    'id': s.id,
                    'symbol': s.symbol,
                    'timestamp': s.timestamp,
                    'entry_time': s.timestamp,
                    'side': s.side,
                    'entry_price': s.entry_price or s.current_price,
                    'current_price': s.current_price,
                    'stop_loss': s.stop_loss,
                    'sl': s.stop_loss,
                    'take_profit_1': s.take_profit_1,
                    'tp1': s.take_profit_1,
                    'take_profit_2': s.take_profit_2,
                    'tp2': s.take_profit_2,
                    'probability': s.probability,
                    'scores': s.scores,
                    'is_prime': s.is_prime,
                })

            session.close()

            print(f"âœ… Loaded {len(signals)} signals from database")
            return signals

        except ImportError:
            print("âš ï¸  Database not available, cannot load signals")
            return []
        except Exception as e:
            print(f"âŒ Failed to load signals from database: {e}")
            import traceback
            traceback.print_exc()
            return []

    def load_price_data(
        self,
        symbols: List[str],
        start_time: datetime,
        end_time: datetime,
        interval: str = '1h',
        use_cache: bool = True
    ) -> Dict[str, List]:
        """
        åŠ è½½ä»·æ ¼æ•°æ®

        Args:
            symbols: å¸ç§åˆ—è¡¨
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            interval: Kçº¿å‘¨æœŸ
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            ä»·æ ¼æ•°æ®å­—å…¸ {symbol: [(timestamp, open, high, low, close, volume), ...]}
        """
        price_data = {}

        for symbol in symbols:
            print(f"ğŸ“Š Loading price data for {symbol}...")

            # æ£€æŸ¥ç¼“å­˜
            if use_cache:
                cached = self._load_from_cache(symbol, start_time, end_time, interval)
                if cached:
                    price_data[symbol] = cached
                    print(f"   âœ… Loaded from cache ({len(cached)} bars)")
                    continue

            # ä»APIè·å–
            try:
                start_ms = int(start_time.timestamp() * 1000)
                end_ms = int(end_time.timestamp() * 1000)

                klines = get_klines(
                    symbol=symbol,
                    interval=interval,
                    limit=1500,
                    start_time=start_ms,
                    end_time=end_ms
                )

                if klines:
                    # è½¬æ¢ä¸ºç®€åŒ–æ ¼å¼ï¼š(timestamp_datetime, open, high, low, close, volume)
                    bars = []
                    for k in klines:
                        timestamp = datetime.fromtimestamp(int(k[0]) / 1000)
                        open_price = float(k[1])
                        high = float(k[2])
                        low = float(k[3])
                        close = float(k[4])
                        volume = float(k[5])

                        bars.append((timestamp, open_price, high, low, close, volume))

                    price_data[symbol] = bars

                    # ä¿å­˜åˆ°ç¼“å­˜
                    if use_cache:
                        self._save_to_cache(symbol, start_time, end_time, interval, bars)

                    print(f"   âœ… Loaded from API ({len(bars)} bars)")
                else:
                    print(f"   âš ï¸  No data returned from API")

            except Exception as e:
                print(f"   âŒ Failed to load: {e}")

        return price_data

    def _get_cache_filename(self, symbol: str, start_time: datetime, end_time: datetime, interval: str) -> Path:
        """
        è·å–ç¼“å­˜æ–‡ä»¶å

        Args:
            symbol: å¸ç§
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            interval: å‘¨æœŸ

        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        start_str = start_time.strftime('%Y%m%d')
        end_str = end_time.strftime('%Y%m%d')
        filename = f"{symbol}_{interval}_{start_str}_{end_str}.json"
        return self.cache_dir / filename

    def _load_from_cache(self, symbol: str, start_time: datetime, end_time: datetime, interval: str) -> Optional[List]:
        """
        ä»ç¼“å­˜åŠ è½½æ•°æ®

        Args:
            symbol: å¸ç§
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            interval: å‘¨æœŸ

        Returns:
            ä»·æ ¼æ•°æ®æˆ–None
        """
        cache_file = self._get_cache_filename(symbol, start_time, end_time, interval)

        if not cache_file.exists():
            return None

        try:
            # å°è¯•æ£€æµ‹æ˜¯å¦ä¸º gzip å‹ç¼©æ–‡ä»¶
            import gzip

            # è¯»å–å‰ä¸¤ä¸ªå­—èŠ‚æ£€æµ‹æ˜¯å¦ä¸º gzip æ–‡ä»¶ï¼ˆé­”æœ¯å­—èŠ‚ï¼š0x1f 0x8bï¼‰
            with open(cache_file, 'rb') as f:
                magic = f.read(2)

            if magic == b'\x1f\x8b':
                # gzip å‹ç¼©æ–‡ä»¶
                with gzip.open(cache_file, 'rt', encoding='utf-8') as f:
                    data = json.load(f)
            else:
                # æ™®é€š JSON æ–‡ä»¶
                with open(cache_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

            # è½¬æ¢å›datetimeå¯¹è±¡
            bars = []
            for bar in data:
                timestamp = datetime.fromisoformat(bar[0])
                bars.append((timestamp, bar[1], bar[2], bar[3], bar[4], bar[5]))

            return bars

        except Exception as e:
            print(f"   âš ï¸  Cache read error: {e}, deleting corrupted cache file")
            # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
            try:
                cache_file.unlink()
            except:
                pass
            return None

    def _save_to_cache(self, symbol: str, start_time: datetime, end_time: datetime, interval: str, bars: List):
        """
        ä¿å­˜æ•°æ®åˆ°ç¼“å­˜

        Args:
            symbol: å¸ç§
            start_time: å¼€å§‹æ—¶é—´
            end_time: ç»“æŸæ—¶é—´
            interval: å‘¨æœŸ
            bars: ä»·æ ¼æ•°æ®
        """
        cache_file = self._get_cache_filename(symbol, start_time, end_time, interval)

        try:
            # è½¬æ¢datetimeä¸ºå­—ç¬¦ä¸²
            data = []
            for bar in bars:
                timestamp_str = bar[0].isoformat()
                data.append([timestamp_str, bar[1], bar[2], bar[3], bar[4], bar[5]])

            with open(cache_file, 'w') as f:
                json.dump(data, f)

        except Exception as e:
            print(f"   âš ï¸  Cache write error: {e}")

    def prepare_backtest_data(
        self,
        start_time: datetime,
        end_time: datetime,
        symbols: Optional[List[str]] = None,
        min_probability: Optional[float] = None,
        use_cache: bool = True
    ) -> tuple:
        """
        å‡†å¤‡å›æµ‹æ‰€éœ€çš„æ‰€æœ‰æ•°æ®

        Args:
            start_time: å›æµ‹å¼€å§‹æ—¶é—´
            end_time: å›æµ‹ç»“æŸæ—¶é—´
            symbols: å¸ç§è¿‡æ»¤ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™ä»ä¿¡å·ä¸­æå–ï¼‰
            min_probability: æœ€å°æ¦‚ç‡è¿‡æ»¤
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜

        Returns:
            (signals, price_data) å…ƒç»„
        """
        print("="*70)
        print("  Preparing Backtest Data")
        print("="*70)
        print(f"Period: {start_time.date()} to {end_time.date()}")
        print()

        # 1. åŠ è½½å†å²ä¿¡å·
        signals = self.load_signals_from_database(
            start_time=start_time,
            end_time=end_time,
            symbols=symbols,
            min_probability=min_probability
        )

        if not signals:
            print("\nâš ï¸  No signals found for this period")
            return signals, {}

        # 2. æå–éœ€è¦çš„å¸ç§ï¼ˆå¦‚æœæœªæŒ‡å®šï¼‰
        if not symbols:
            symbols = list(set(s['symbol'] for s in signals))
            print(f"\nğŸ“Š Extracted {len(symbols)} unique symbols from signals")

        # 3. åŠ è½½ä»·æ ¼æ•°æ®
        print(f"\nğŸ“ˆ Loading price data for {len(symbols)} symbols...")
        price_data = self.load_price_data(
            symbols=symbols,
            start_time=start_time,
            end_time=end_time,
            interval='1h',
            use_cache=use_cache
        )

        print()
        print("="*70)
        print(f"âœ… Data preparation completed")
        print(f"   Signals: {len(signals)}")
        print(f"   Symbols: {len(price_data)}")
        print("="*70)
        print()

        return signals, price_data
